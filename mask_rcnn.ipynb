{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn.config import Config\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(Config):\n",
    "    NAME = \"coco\"\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 1 + 80  # COCO dataset has 80 classes + 1 background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(weights_path):\n",
    "    config = InferenceConfig()\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir='./', config=config)\n",
    "    model.load_weights(weights_path, by_name=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = [(68, 136, 229), (32, 67, 6), (169, 77, 36), (240, 81, 131), (164, 229, 229), (92, 21, 220), (189, 226, 125), (248, 19, 241), (15, 249, 52), (85, 150, 138), (148, 194, 1), (152, 143, 116), (217, 58, 224), (55, 178, 45), (243, 94, 206), (233, 51, 152), (132, 172, 38), (110, 253, 131), (45, 29, 43), (111, 100, 13), (37, 254, 130), (96, 61, 69), (88, 207, 28), (181, 120, 144), (20, 3, 192), (233, 102, 128), (156, 176, 34), (152, 170, 43), (78, 136, 191), (77, 118, 249), (173, 214, 194), (173, 118, 231), (185, 254, 208), (182, 58, 250), (158, 226, 80), (154, 160, 218), (53, 238, 212), (79, 212, 233), (38, 13, 25), (62, 165, 205), (144, 58, 51), (34, 98, 57), (110, 47, 230), (192, 45, 5), (6, 84, 105), (8, 176, 71), (56, 189, 201), (53, 71, 4), (44, 201, 184), (93, 150, 19), (135, 216, 229), (181, 96, 93), (104, 94, 68), (24, 239, 200), (102, 66, 48), (192, 136, 10), (207, 180, 13), (185, 226, 128), (51, 133, 65), (144, 4, 93), (121, 224, 191), (12, 95, 122), (226, 74, 204), (17, 190, 251), (233, 36, 103), (46, 224, 233), (214, 104, 188), (22, 35, 154), (85, 152, 137), (194, 204, 8), (174, 199, 102), (125, 102, 195), (39, 172, 16), (69, 112, 131), (191, 65, 22), (251, 66, 45), (154, 68, 24), (7, 3, 77), (232, 83, 131), (212, 33, 50), (16, 104, 28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_COLORS = {i: colours[i] for i in range(len(colours))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnames = {0: u'__background__',\n",
    " 1: u'person',\n",
    " 2: u'bicycle',\n",
    " 3: u'car',\n",
    " 4: u'motorcycle',\n",
    " 5: u'airplane',\n",
    " 6: u'bus',\n",
    " 7: u'train',\n",
    " 8: u'truck',\n",
    " 9: u'boat',\n",
    " 10: u'traffic light',\n",
    " 11: u'fire hydrant',\n",
    " 12: u'stop sign',\n",
    " 13: u'parking meter',\n",
    " 14: u'bench',\n",
    " 15: u'bird',\n",
    " 16: u'cat',\n",
    " 17: u'dog',\n",
    " 18: u'horse',\n",
    " 19: u'sheep',\n",
    " 20: u'cow',\n",
    " 21: u'elephant',\n",
    " 22: u'bear',\n",
    " 23: u'zebra',\n",
    " 24: u'giraffe',\n",
    " 25: u'backpack',\n",
    " 26: u'umbrella',\n",
    " 27: u'handbag',\n",
    " 28: u'tie',\n",
    " 29: u'suitcase',\n",
    " 30: u'frisbee',\n",
    " 31: u'skis',\n",
    " 32: u'snowboard',\n",
    " 33: u'sports ball',\n",
    " 34: u'kite',\n",
    " 35: u'baseball bat',\n",
    " 36: u'baseball glove',\n",
    " 37: u'skateboard',\n",
    " 38: u'surfboard',\n",
    " 39: u'tennis racket',\n",
    " 40: u'bottle',\n",
    " 41: u'wine glass',\n",
    " 42: u'cup',\n",
    " 43: u'fork',\n",
    " 44: u'knife',\n",
    " 45: u'spoon',\n",
    " 46: u'bowl',\n",
    " 47: u'banana',\n",
    " 48: u'apple',\n",
    " 49: u'sandwich',\n",
    " 50: u'orange',\n",
    " 51: u'broccoli',\n",
    " 52: u'carrot',\n",
    " 53: u'hot dog',\n",
    " 54: u'pizza',\n",
    " 55: u'donut',\n",
    " 56: u'cake',\n",
    " 57: u'chair',\n",
    " 58: u'couch',\n",
    " 59: u'potted plant',\n",
    " 60: u'bed',\n",
    " 61: u'dining table',\n",
    " 62: u'toilet',\n",
    " 63: u'tv',\n",
    " 64: u'laptop',\n",
    " 65: u'mouse',\n",
    " 66: u'remote',\n",
    " 67: u'keyboard',\n",
    " 68: u'cell phone',\n",
    " 69: u'microwave',\n",
    " 70: u'oven',\n",
    " 71: u'toaster',\n",
    " 72: u'sink',\n",
    " 73: u'refrigerator',\n",
    " 74: u'book',\n",
    " 75: u'clock',\n",
    " 76: u'vase',\n",
    " 77: u'scissors',\n",
    " 78: u'teddy bear',\n",
    " 79: u'hair drier',\n",
    " 80: u'toothbrush'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply Mask R-CNN to each frame and overlay results\n",
    "def process_frame(model, frame):\n",
    "    results = model.detect([frame], verbose=0)\n",
    "    r = results[0]\n",
    "    \n",
    "    # Create a blank canvas for segmented mask\n",
    "    mask_canvas = np.zeros_like(frame)\n",
    "    \n",
    "    # Apply detected masks\n",
    "    for i in range(r['masks'].shape[-1]):\n",
    "        mask = r['masks'][:, :, i]\n",
    "        class_id = r['class_ids'][i]\n",
    "        color = CATEGORY_COLORS.get(class_id-1, tuple(np.random.randint(0, 255, (3,), dtype=int)))\n",
    "        mask_canvas[mask] = color\n",
    "    \n",
    "    # Overlay mask on the original frame\n",
    "    alpha = 0.5  # Transparency factor\n",
    "    segmented_frame = cv2.addWeighted(frame, 1 - alpha, mask_canvas, alpha, 0)\n",
    "    return segmented_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_legend_canvas(category_colors, canvas_height, max_rows_per_column=20):\n",
    "    \"\"\"\n",
    "    Creates a canvas with a legend showing category colors and labels in multiple columns.\n",
    "    \"\"\"\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.5\n",
    "    thickness = 1\n",
    "    box_height = 20\n",
    "    line_height = 30\n",
    "    box_width = 30\n",
    "    text_offset = 10\n",
    "    \n",
    "    # Calculate the number of columns needed\n",
    "    total_categories = len(category_colors)\n",
    "    num_columns = -(-total_categories // max_rows_per_column)  # Ceiling division\n",
    "    canvas_width = num_columns * 200  # Adjust column width as needed\n",
    "    \n",
    "    # Create a white canvas\n",
    "    canvas = np.ones((canvas_height, canvas_width, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    # Draw the legend\n",
    "    y_offset = 20  # Initial offset from the top\n",
    "    x_offset = 10  # Initial offset from the left\n",
    "    current_row = 0\n",
    "    \n",
    "    for category_id, color in category_colors.items():\n",
    "        # Start a new column if max rows reached\n",
    "        if current_row >= max_rows_per_column:\n",
    "            current_row = 0\n",
    "            x_offset += 200\n",
    "            y_offset = 20\n",
    "\n",
    "        # Draw the color box\n",
    "        box_start = (x_offset, y_offset)\n",
    "        box_end = (x_offset + box_width, y_offset + box_height)\n",
    "        cv2.rectangle(canvas, box_start, box_end, color[::-1], -1)  # OpenCV uses BGR\n",
    "        \n",
    "        # Draw the category label\n",
    "        label = f\"{classnames[category_id-1]}\"\n",
    "        text_position = (x_offset + box_width + text_offset, y_offset + 15)\n",
    "        cv2.putText(canvas, label, text_position, font, font_scale, (0, 0, 0), thickness, cv2.LINE_AA)\n",
    "        \n",
    "        # Increment row\n",
    "        y_offset += line_height\n",
    "        current_row += 1\n",
    "    \n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def process_and_display_video(input_video_path, model, category_colors, max_duration_seconds=1):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    max_frames = int(fps * max_duration_seconds)\n",
    "    \n",
    "    # Lists to store original and segmented frames\n",
    "    original_frames = []\n",
    "    segmented_frames = []\n",
    "    frame_count = 0\n",
    "    \n",
    "    print(\"Processing video...\")\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize frame for inference\n",
    "        resized_frame = cv2.resize(frame, (1024, 1024))\n",
    "        segmented_frame = process_frame(model, resized_frame)\n",
    "        \n",
    "        # Resize segmented frame back to original dimensions\n",
    "        segmented_frame = cv2.resize(segmented_frame, (width, height))\n",
    "        \n",
    "        # Store frames in memory\n",
    "        original_frames.append(frame)\n",
    "        segmented_frames.append(segmented_frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    print(\"Processing complete! Displaying video...\")\n",
    "\n",
    "    # Create a legend canvas\n",
    "    legend_canvas = create_legend_canvas(category_colors, height)\n",
    "    \n",
    "    while True:\n",
    "        for original, segmented in zip(original_frames, segmented_frames):\n",
    "            # Combine the segmented frame with the legend\n",
    "            combined_segmented = np.hstack((segmented, legend_canvas))\n",
    "            \n",
    "            # Combine the frames horizontally\n",
    "            combined_frame = np.hstack((original, combined_segmented))\n",
    "            \n",
    "            # Display the combined frame\n",
    "            cv2.imshow(\"Original, Segmented Video, and Legend\", combined_frame)\n",
    "            \n",
    "            # Exit on pressing 'q'\n",
    "            key = cv2.waitKey(int(1000 / fps))\n",
    "            if key & 0xFF == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                print(\"Video display complete!\")\n",
    "                return\n",
    "    \n",
    "    # Release all OpenCV windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:442: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:58: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3543: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3386: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1768: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1154: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1188: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\model.py:399: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\model.py:722: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\mask_rcnn-2.1-py3.6.egg\\mrcnn\\model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:153: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:158: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:163: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:333: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\maskrcnn\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:341: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "weights_path = \"mask_rcnn_coco.h5\"  # Update with your path\n",
    "model = load_model(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video...\n",
      "Processing complete! Displaying video...\n",
      "Video display complete!\n"
     ]
    }
   ],
   "source": [
    "# Process the input video\n",
    "input_video_path = \"videoplayback.mp4\"\n",
    "process_and_display_video(input_video_path, model, CATEGORY_COLORS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maskrcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
